<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Y&amp;F</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="The last theme you&#39;ll ever need. Maybe.">
    <meta name="generator" content="Hugo 0.141.0">
    
    
    
      <meta name="robots" content="index, follow">
    
    
      <meta name="author" content = "钱甫新">
    

    
<link rel="stylesheet" href="/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css" >



    

    
      

    

    
    
      <link href="/tags/ollama/index.xml" rel="alternate" type="application/rss+xml" title="Y&amp;F" />
      <link href="/tags/ollama/index.xml" rel="feed" type="application/rss+xml" title="Y&amp;F" />
      
    

    
      <link rel="canonical" href="https://qianfuxin.gitHub.io/tags/ollama/">
    

    <meta property="og:url" content="https://qianfuxin.gitHub.io/tags/ollama/">
  <meta property="og:site_name" content="Y&F">
  <meta property="og:title" content="Ollama">
  <meta property="og:description" content="The last theme you&#39;ll ever need. Maybe.">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="website">

  <meta itemprop="name" content="Ollama">
  <meta itemprop="description" content="The last theme you&#39;ll ever need. Maybe.">
  <meta itemprop="datePublished" content="2025-05-24T10:58:08+00:00">
  <meta itemprop="dateModified" content="2025-05-24T10:58:08+00:00">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Ollama">
  <meta name="twitter:description" content="The last theme you&#39;ll ever need. Maybe.">

      
    
	
  </head><body class="ma0 avenir bg-near-white production">

    

  <header>
    <div class="pb3-m pb6-l bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      
        Y&amp;F
      
    </a>
    <div class="flex-l items-center">
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="关于我 页">
              关于我
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="文章 页">
              文章
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

      <div class="tc-l pv3 ph3 ph4-ns">
        <h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">
          Ollama
        </h1>
        
      </div>
    </div>
  </header>


    <main class="pb7" role="main">
      
  <article class="cf pa3 pa4-m pa4-l">
    <div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray">
      <p>标签为“Ollama”的页面如下</p>
    </div>
  </article>
  <div class="mw8 center">
    <section class="flex-ns flex-wrap justify-around mt5">
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        五月 24, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/ollama%E4%BD%BF%E7%94%A8%E5%B0%8F%E5%8F%82%E6%95%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E4%BB%BB%E5%8A%A1/" class="link black dim">
        ollama使用小参数大模型实现简单的任务
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h1 id="实际意义">实际意义</h1>
<p>很多场景下都是简单的任务，如果都用大参数大模型去处理，并发、资源、耗时都是问题。</p>
<p>所以对于某些简单的任务，可以让小参数大模型去做，不要精确率多高，只要有效，哪怕百分之十，配上程序的逻辑判断，都可以为生产做贡献。</p>
<h1 id="构建">构建</h1>
<h2 id="modelfile">modelfile</h2>
<pre tabindex="0"><code>FROM qwen3:0.6b
# 设置温度，较低的温度使输出更具确定性和一致性，适合分类任务
# 对于小模型，较低的温度有助于减少不相关或错误的输出
PARAMETER temperature 0.1

# 系统提示：明确告知模型其角色和任务，以及预期的输出类别
# 对于小模型，指令需要非常直接和清晰
SYSTEM 你是一个文本情绪分析助手。请将用户输入的文本分类为“积极”、“消极”或“中性”。

# 少样本提示：提供清晰的输入输出样例，帮助模型理解任务
# 样例要简单直接，符合小模型的理解能力
MESSAGE user 今天阳光明媚，我心情很好！
MESSAGE assistant 积极
MESSAGE user 我的项目失败了，感觉很难过。
MESSAGE assistant 消极
MESSAGE user 会议将在下午三点开始。
MESSAGE assistant 中性
MESSAGE user 这家餐厅的食物味道一般。
MESSAGE assistant 中性
</code></pre><h2 id="构建-1">构建</h2>
<p><code>ollama create emotion_cls -f ./Modelfile</code></p>
<h2 id="使用">使用</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/generate -d <span style="color:#e6db74">&#39;{ &#34;model&#34;: &#34;emotion_cls&#34;,&#34;prompt&#34;:&#34;哈哈哈哈&#34;, &#34;stream&#34;: false}&#39;</span> | jq .
</span></span></code></pre></div><p>不使用思考模式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/generate -d <span style="color:#e6db74">&#39;{ &#34;model&#34;: &#34;emotion_cls&#34;,&#34;prompt&#34;:&#34;哈哈哈哈/no_think&#34;, &#34;stream&#34;: false}&#39;</span> | jq .
</span></span></code></pre></div>
    </div>
  <a href="/post/ollama%E4%BD%BF%E7%94%A8%E5%B0%8F%E5%8F%82%E6%95%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0%E7%AE%80%E5%8D%95%E7%9A%84%E4%BB%BB%E5%8A%A1/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">继续阅读</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        三月 8, 2025
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/%E5%AE%9E%E7%8E%B0rag%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/" class="link black dim">
        实现rag的几种方式
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h1 id="ollama提供embeddingsllm">ollama提供embeddings、llm</h1>
<ul>
<li>llm可以是ollama，或者接口为openai格式即可</li>
<li>embeddings服务由ollama提供</li>
<li>向量数据库容器提供（cpu）</li>
</ul>
<p><a href="https://gist.github.com/QianFuXin/39ba41e29a58e7b9b1636db06d774237">源码</a></p>
<h1 id="ollama提供llm容器提供embeddings">ollama提供llm，容器提供embeddings</h1>
<ul>
<li>llm可以是ollama，或者接口为openai格式即可</li>
<li>embeddings服务由HuggingFaceEmbeddings提供</li>
<li>向量数据库容器提供（cpu）</li>
</ul>
<p><a href="https://gist.github.com/QianFuXin/0e94c3875574f0315af9fe58a7b9b04e">源码</a></p>
<h1 id="llmembeddings向量数据库均由第三方提供">llm、embeddings、向量数据库均由第三方提供</h1>
<p><a href="https://gist.github.com/QianFuXin/78945087cab1e8321138025f307d570d">源码</a></p>
<h1 id="llmembeddings向量数据库关键字搜索es重排序均由第三方提供">llm、embeddings、向量数据库、关键字搜索（es）、重排序、均由第三方提供</h1>
<p><a href="https://gist.github.com/QianFuXin/31549fb7cdef55c956de0aeff817c19c">源码</a></p>
    </div>
  <a href="/post/%E5%AE%9E%E7%8E%B0rag%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">继续阅读</a>
  </div>

        </div>
      
        <div class="relative w-100  mb4 bg-white">
            <div class="mb3 pa4 mid-gray overflow-hidden">
    
      <div class="f6">
        九月 24, 2024
      </div>
    
    <h1 class="f3 near-black">
      <a href="/post/pythongui%E5%AE%9E%E7%8E%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E5%BC%8F%E5%AF%B9%E8%AF%9D_whisper_ollama/" class="link black dim">
        pythonGUI实现大模型流式对话_whisper_ollama
      </a>
    </h1>
    <div class="nested-links f5 lh-copy nested-copy-line-height">
      <h1 id="服务部署">服务部署</h1>
<h2 id="部署whisper-api服务">部署whisper api服务</h2>
<pre><code>version: '3.8'

services:
  whisper-asr:
    image: onerahmet/openai-whisper-asr-webservice:latest
    container_name: whisper-asr-service
    ports:
      - &quot;9099:9000&quot;
    volumes:
      - /Users/qianfuxin/dc/whisper:/root/.cache/whisper
    environment:
      - ASR_MODEL=base
      - ASR_ENGINE=openai_whisper
    restart: unless-stopped
</code></pre>
<h2 id="部署ollama">部署ollama</h2>
<p>见官网</p>
<h1 id="代码">代码</h1>
<pre><code>import mimetypes
import os
import sys
from datetime import datetime
import socksio
import requests
from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QTextEdit
import pyaudio
import wave
import threading
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI


def transcribe_audio(file_path):
    &quot;&quot;&quot;
    Function to transcribe audio using the ASR service.

    :param file_path: The full path to the audio file to be transcribed.
    :return: The response text from the ASR service.
    &quot;&quot;&quot;
    # Ensure the file exists
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f&quot;Audio file not found at: {file_path}&quot;)

    # Determine MIME type
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type is None:
        raise ValueError(f&quot;Unable to determine MIME type for file: {file_path}&quot;)

    # Extract file name
    file_name = os.path.basename(file_path)

    url = &quot;http://localhost:9099/asr&quot;
    params = {
        &quot;encode&quot;: &quot;true&quot;,
        &quot;task&quot;: &quot;transcribe&quot;,
        &quot;language&quot;: &quot;zh&quot;,
        &quot;initial_prompt&quot;: &quot;这是一段简体中文&quot;,
        &quot;word_timestamps&quot;: &quot;false&quot;,
        &quot;output&quot;: &quot;txt&quot;
    }
    headers = {
        &quot;accept&quot;: &quot;application/json&quot;,
    }

    # Open the audio file
    with open(file_path, &quot;rb&quot;) as audio_file:
        files = {
            &quot;audio_file&quot;: (file_name, audio_file, mime_type)
        }

        # Send the POST request
        response = requests.post(url, headers=headers, params=params, files=files)
    return response.text


def get_model(base_url=&quot;http://127.0.0.1:11434/v1&quot;):
    &quot;&quot;&quot;Get the AI model instance.&quot;&quot;&quot;
    model = ChatOpenAI(
        api_key=&quot;ollama&quot;,
        model=&quot;qwen2&quot;,
        base_url=base_url
    )
    return model


class AudioRecorder:
    def __init__(self):
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.frames = []
        self.device_index = None
        self.is_recording = False
        self.recording_thread = None

    def list_input_devices(self):
        &quot;&quot;&quot;List available input devices.&quot;&quot;&quot;
        device_list = []
        for i in range(self.p.get_device_count()):
            device_info = self.p.get_device_info_by_index(i)
            if device_info[&quot;maxInputChannels&quot;] &gt; 0:
                device_list.append((i, device_info[&quot;name&quot;]))
        return device_list

    def start_recording(self, device_index):
        &quot;&quot;&quot;Start recording with the specified device.&quot;&quot;&quot;
        self.device_index = device_index
        self.frames = []
        self.is_recording = True

        self.stream = self.p.open(format=pyaudio.paInt16,
                                  channels=1,
                                  rate=44100,
                                  input=True,
                                  input_device_index=device_index,
                                  frames_per_buffer=1024)

        self.recording_thread = threading.Thread(target=self.record)
        self.recording_thread.start()

    def record(self):
        &quot;&quot;&quot;Record audio in a separate thread.&quot;&quot;&quot;
        try:
            while self.is_recording:
                data = self.stream.read(1024, exception_on_overflow=False)
                self.frames.append(data)
        except IOError as e:
            print(f&quot;Buffer overflow occurred: {e}&quot;)

    def stop_recording(self, filename):
        &quot;&quot;&quot;Stop recording and save the file.&quot;&quot;&quot;
        if self.is_recording:
            self.is_recording = False
            if self.recording_thread and self.recording_thread.is_alive():
                self.recording_thread.join()

            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
                self.stream = None

            with wave.open(filename, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(self.p.get_sample_size(pyaudio.paInt16))
                wf.setframerate(44100)
                wf.writeframes(b''.join(self.frames))

    def close(self):
        &quot;&quot;&quot;Close the PyAudio instance.&quot;&quot;&quot;
        self.p.terminate()


class RecorderApp(QWidget):
    def __init__(self):
        super().__init__()

        self.recorder = AudioRecorder()
        self.filename = None
        self.model = get_model()
        self.init_ui()

    def init_ui(self):
        self.setWindowTitle(&quot;实时语音识别与问答&quot;)
        self.setGeometry(100, 100, 500, 400)

        layout = QVBoxLayout()

        # 开启录音按钮
        self.start_button = QPushButton(&quot;开启录音&quot;)
        self.start_button.clicked.connect(self.start_recording)
        layout.addWidget(self.start_button)

        # 结束录音按钮
        self.stop_button = QPushButton(&quot;结束录音&quot;)
        self.stop_button.clicked.connect(self.stop_recording)
        self.stop_button.setEnabled(False)
        layout.addWidget(self.stop_button)

        # 状态显示标签
        self.status_label = QLabel(&quot;用户问题: &quot;, self)
        layout.addWidget(self.status_label)

        # 输出结果框
        self.output_box = QTextEdit(self)
        self.output_box.setReadOnly(True)
        layout.addWidget(self.output_box)

        self.setLayout(layout)

    def start_recording(self):
        device_index = 0  # For simplicity, we assume default device.
        start_time = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
        self.filename = f&quot;{start_time}.wav&quot;

        self.recorder.start_recording(device_index)
        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)

    def stop_recording(self):
        self.recorder.stop_recording(self.filename)
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)

        # 转录音频并生成回答
        transcription = transcribe_audio(self.filename)
        self.status_label.setText(f&quot;用户问题: {transcription}&quot;)
        self.generate_answer(transcription)

    def generate_answer(self, transcription):
        &quot;&quot;&quot;
        Send the transcription to the large model and display the streamed response.
        &quot;&quot;&quot;
        QApplication.processEvents()

        messages = [
            SystemMessage(&quot;你是一个智能助手。&quot;),
            HumanMessage(content=transcription)
        ]
        parser = StrOutputParser()
        chain = self.model | parser

        response = &quot;&quot;
        for token in chain.stream(messages):
            response += token
            self.output_box.setText(response)
            QApplication.processEvents()


if __name__ == &quot;__main__&quot;:
    app = QApplication(sys.argv)
    window = RecorderApp()
    window.show()
    sys.exit(app.exec_())
</code></pre>
<h3 id="仅仅实现asr">仅仅实现ASR</h3>
<pre><code>import mimetypes
import os
import sys
from datetime import datetime

import requests
from PyQt5.QtGui import QIcon
from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QLineEdit, QComboBox, QMessageBox, \
    QSystemTrayIcon, QMenu, QAction
import pyaudio
import wave
import threading


def transcribe_audio(file_path):
    &quot;&quot;&quot;
    Function to transcribe audio using the ASR service.

    :param file_path: The full path to the audio file to be transcribed.
    :return: The response text from the ASR service.
    &quot;&quot;&quot;
    # Ensure the file exists
    if not os.path.isfile(file_path):
        raise FileNotFoundError(f&quot;Audio file not found at: {file_path}&quot;)

    # Determine MIME type
    mime_type, _ = mimetypes.guess_type(file_path)
    if mime_type is None:
        raise ValueError(f&quot;Unable to determine MIME type for file: {file_path}&quot;)

    # Extract file name
    file_name = os.path.basename(file_path)

    url = &quot;http://localhost:9099/asr&quot;
    params = {
        &quot;encode&quot;: &quot;true&quot;,
        &quot;task&quot;: &quot;transcribe&quot;,
        &quot;language&quot;: &quot;zh&quot;,
        &quot;initial_prompt&quot;: &quot;这是一段简体中文&quot;,
        &quot;word_timestamps&quot;: &quot;false&quot;,
        &quot;output&quot;: &quot;txt&quot;
    }
    headers = {
        &quot;accept&quot;: &quot;application/json&quot;,
    }

    # Open the audio file
    with open(file_path, &quot;rb&quot;) as audio_file:
        files = {
            &quot;audio_file&quot;: (file_name, audio_file, mime_type)
        }

        # Send the POST request
        response = requests.post(url, headers=headers, params=params, files=files)

    return response.text


class AudioRecorder:
    def __init__(self):
        self.p = pyaudio.PyAudio()
        self.stream = None
        self.frames = []
        self.device_index = None
        self.is_recording = False
        self.recording_thread = None

    def list_input_devices(self):
        &quot;&quot;&quot;列出所有可用的录音设备&quot;&quot;&quot;
        device_list = []
        for i in range(self.p.get_device_count()):
            device_info = self.p.get_device_info_by_index(i)
            # 筛选出录音设备
            if device_info[&quot;maxInputChannels&quot;] &gt; 0:
                device_list.append((i, device_info[&quot;name&quot;]))
        return device_list

    def start_recording(self, device_index):
        &quot;&quot;&quot;使用指定设备开始录音&quot;&quot;&quot;
        self.device_index = device_index
        self.frames = []
        self.is_recording = True

        self.stream = self.p.open(format=pyaudio.paInt16,
                                  channels=1,
                                  rate=44100,
                                  input=True,
                                  input_device_index=device_index,
                                  frames_per_buffer=1024)

        # 启动录音线程
        self.recording_thread = threading.Thread(target=self.record)
        self.recording_thread.start()

    def record(self):
        &quot;&quot;&quot;录音线程函数，用于捕获音频数据&quot;&quot;&quot;
        try:
            while self.is_recording:
                data = self.stream.read(1024, exception_on_overflow=False)
                self.frames.append(data)
        except IOError as e:
            print(f&quot;Buffer overflow occurred: {e}&quot;)

    def stop_recording(self, filename):
        &quot;&quot;&quot;停止录音并保存文件&quot;&quot;&quot;
        if self.is_recording:
            self.is_recording = False
            if self.recording_thread and self.recording_thread.is_alive():
                self.recording_thread.join()

            if self.stream:
                self.stream.stop_stream()
                self.stream.close()
                self.stream = None

            with wave.open(filename, 'wb') as wf:
                wf.setnchannels(1)
                wf.setsampwidth(self.p.get_sample_size(pyaudio.paInt16))
                wf.setframerate(44100)
                wf.writeframes(b''.join(self.frames))

    def close(self):
        &quot;&quot;&quot;关闭 PyAudio 实例&quot;&quot;&quot;
        self.p.terminate()


def resource_path(relative_path):
    &quot;&quot;&quot;获取资源文件的绝对路径&quot;&quot;&quot;
    if hasattr(sys, '_MEIPASS'):
        return os.path.join(sys._MEIPASS, relative_path)
    return os.path.join(os.path.abspath(&quot;.&quot;), relative_path)


class RecorderApp(QWidget):
    def __init__(self):
        super().__init__()

        self.recorder = AudioRecorder()
        self.filename = None

        self.init_ui()

    def init_ui(self):
        self.setWindowTitle(&quot;录音器&quot;)
        self.setGeometry(100, 100, 400, 300)

        # 创建系统托盘图标
        self.tray_icon = QSystemTrayIcon(self)
        self.tray_icon.setIcon(QIcon(resource_path(&quot;icon.png&quot;)))

        # 创建托盘菜单
        tray_menu = QMenu()

        # 添加显示窗口的选项
        show_action = QAction(&quot;显示窗口&quot;, self)
        show_action.triggered.connect(self.show)
        tray_menu.addAction(show_action)

        # 添加退出的选项
        exit_action = QAction(&quot;退出&quot;, self)
        exit_action.triggered.connect(QApplication.instance().quit)
        tray_menu.addAction(exit_action)

        # 将菜单添加到托盘图标
        self.tray_icon.setContextMenu(tray_menu)

        # 设置点击双击托盘图标的动作
        self.tray_icon.activated.connect(self.on_tray_icon_activated)

        # 显示托盘图标
        self.tray_icon.show()

        layout = QVBoxLayout()

        # 开启录音按钮
        self.start_button = QPushButton(&quot;开启录音&quot;)
        self.start_button.clicked.connect(self.start_recording)
        layout.addWidget(self.start_button)

        # 结束录音按钮
        self.stop_button = QPushButton(&quot;结束录音&quot;)
        self.stop_button.clicked.connect(self.stop_recording)
        self.stop_button.setEnabled(False)
        layout.addWidget(self.stop_button)

        # 设备选择下拉菜单
        self.device_selector = QComboBox(self)
        devices = self.recorder.list_input_devices()
        if devices:
            for index, name in devices:
                self.device_selector.addItem(name, index)
        else:
            QMessageBox.warning(self, &quot;错误&quot;, &quot;未检测到录音设备&quot;)
            self.start_button.setEnabled(False)
        layout.addWidget(self.device_selector)

        # 状态显示标签
        self.status_label = QLabel(&quot;状态: 未录音&quot;, self)
        layout.addWidget(self.status_label)

        self.setLayout(layout)

    def closeEvent(self, event):
        &quot;&quot;&quot;窗口关闭事件&quot;&quot;&quot;
        if self.isVisible():  # 如果窗口可见，则最小化到托盘
            event.ignore()
            self.hide()
            self.tray_icon.showMessage(
                &quot;应用最小化&quot;, &quot;程序已最小化到托盘&quot;,
                QSystemTrayIcon.Information, 2000
            )
        else:  # 关闭应用时清理资源
            self.recorder.close()
            event.accept()

    def on_tray_icon_activated(self, reason):
        &quot;&quot;&quot;托盘图标激活事件&quot;&quot;&quot;
        if reason == QSystemTrayIcon.DoubleClick:
            self.show()

    def start_recording(self):
        device_index = self.device_selector.currentData()
        start_time = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
        self.filename = f&quot;{start_time}.wav&quot;

        self.recorder.start_recording(device_index)
        self.status_label.setText(&quot;状态: 录音中...&quot;)
        self.start_button.setEnabled(False)
        self.stop_button.setEnabled(True)

    def stop_recording(self):
        end_time = datetime.now().strftime(&quot;%Y%m%d_%H%M%S&quot;)
        self.filename = f&quot;{self.filename[:-4]}-{end_time}.wav&quot;  # 更新文件名
        self.recorder.stop_recording(self.filename)
        self.status_label.setText(f&quot;状态: 录音已保存为 {self.filename}&quot;)
        self.start_button.setEnabled(True)
        self.stop_button.setEnabled(False)
        self.status_label.setText(f&quot;状态: 录音已保存为 {self.filename}\n录音内容：{transcribe_audio(self.filename)}&quot;)


# 主程序入口
if __name__ == &quot;__main__&quot;:
    app = QApplication(sys.argv)
    window = RecorderApp()
    window.show()
    sys.exit(app.exec_())
    &quot;&quot;&quot;
    pyinstaller --onefile --windowed --noconfirm --add-data &quot;icon.png;.&quot; app.py
    &quot;&quot;&quot;
</code></pre>
    </div>
  <a href="/post/pythongui%E5%AE%9E%E7%8E%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%B5%81%E5%BC%8F%E5%AF%B9%E8%AF%9D_whisper_ollama/" class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">继续阅读</a>
  </div>

        </div>
      
    </section>
  </div>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://qianfuxin.gitHub.io/" >
    &copy;  Y&F 2025 
  </a>
    <div><div class="ananke-socials"></div>
</div>
  </div>
</footer>

  </body>
</html>
